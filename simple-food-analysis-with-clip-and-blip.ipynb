{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Enviornment Setup\n","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Check Cuda","metadata":{}},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():\n    print(\"CUDA is available.\")\nelse:\n    print(\"No CUDA found.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Database Building and Analysis","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torch\nfrom transformers import (\n    CLIPModel,\n    CLIPProcessor,\n    BlipProcessor,\n    BlipForConditionalGeneration\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nclass SimpleImageDatabase:\n    def __init__(self, clip_model, clip_processor, device):\n        self.clip_model = clip_model\n        self.clip_processor = clip_processor\n        self.device = device\n        self.image_paths = []\n        self.embeddings = []\n        self.metadata = []\n\n    def add_image(self, image_path, metadata=None):\n        try:\n            img = Image.open(image_path).convert(\"RGB\")\n            inputs = self.clip_processor(images=img, return_tensors=\"pt\").to(self.device)\n            with torch.no_grad():\n                features = self.clip_model.get_image_features(**inputs)\n            features = features / features.norm(dim=-1, keepdim=True)\n            self.image_paths.append(image_path)\n            self.embeddings.append(features.squeeze().cpu().numpy())\n            self.metadata.append(metadata or {})\n        except Exception as e:\n            print(f\"Error {image_path}: {e}\")\n\n    def search_similar(self, query_image_path, top_k=5):\n        try:\n            query_img = Image.open(query_image_path).convert(\"RGB\")\n            inputs = self.clip_processor(images=query_img, return_tensors=\"pt\").to(self.device)\n            with torch.no_grad():\n                query_features = self.clip_model.get_image_features(**inputs)\n            query_features = query_features / query_features.norm(dim=-1, keepdim=True)\n            q_emb = query_features.squeeze().cpu().numpy()\n\n            similarities = []\n            for idx, emb in enumerate(self.embeddings):\n                sim_v = float(np.dot(q_emb, emb))\n                similarities.append((idx, sim_v))\n            similarities.sort(key=lambda x: x[1], reverse=True)\n\n            results = []\n            for i, sim_val in similarities[:top_k]:\n                results.append({\n                    \"path\": self.image_paths[i],\n                    \"similarity\": sim_val,\n                    \"metadata\": self.metadata[i]\n                })\n            return results\n        except Exception as e:\n            print(f\"Search error: {e}\")\n            return []\n\ndef build_image_database(dataset_path, subset_dir, clip_model, clip_processor, nutrition_df, subset_size=5000, device=device):\n    all_imgs = glob.glob(os.path.join(dataset_path, \"images\", \"*\", \"*.jpg\"))\n    if subset_size < len(all_imgs):\n        chosen_files = random.sample(all_imgs, subset_size)\n    else:\n        chosen_files = all_imgs\n\n    os.makedirs(subset_dir, exist_ok=True)\n    for path in chosen_files:\n        category_folder = os.path.basename(os.path.dirname(path))\n        local_dir = os.path.join(subset_dir, category_folder)\n        os.makedirs(local_dir, exist_ok=True)\n        dest = os.path.join(local_dir, os.path.basename(path))\n        if not os.path.exists(dest):\n            try:\n                os.system(f'cp \"{path}\" \"{dest}\"')\n            except Exception as err:\n                print(f\"Copy error: {err}\")\n\n    db = SimpleImageDatabase(clip_model, clip_processor, device)\n    final_imgs = glob.glob(os.path.join(subset_dir, \"*\", \"*.jpg\"))\n    for img_path in final_imgs:\n        cat_name = os.path.basename(os.path.dirname(img_path))\n        meta = {\"food_category\": cat_name}\n        row = nutrition_df[nutrition_df[\"food_name\"] == cat_name.lower()]\n        if not row.empty:\n            row_data = row.iloc[0].to_dict()\n            meta.update(row_data)\n        db.add_image(img_path, meta)\n    return db\n\ndef analyze_food_image(image_path, blip_model, blip_processor, similar_imgs, device=device):\n    try:\n        img = Image.open(image_path).convert(\"RGB\")\n        inputs = blip_processor(img, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            cap_ids = blip_model.generate(**inputs, max_new_tokens=50)\n        caption = blip_processor.decode(cap_ids[0], skip_special_tokens=True)\n    except Exception as e:\n        caption = f\"Error: {e}\"\n\n    if similar_imgs:\n        top_candidate = similar_imgs[0]\n        cat = top_candidate[\"metadata\"].get(\"food_category\", \"Unknown\")\n        desc = top_candidate[\"metadata\"].get(\"description\", \"No description\")\n    else:\n        cat = \"Unknown\"\n        desc = \"No match\"\n    return f\"\\nImage: {os.path.basename(image_path)}\\nCaption: {caption}\\nCategory: {cat}\\nInfo: {desc}\"\n\ndef main():\n    parent_dataset_path = \"/kaggle/input/food101/food-101\"\n    local_subset_path = \"/kaggle/working/food101_subset\"\n    clip_m = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n    clip_p = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n    blip_proc = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n    blip_mod = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n\n    data_map = {\n        \"food_name\": [\"apple_pie\", \"pizza\", \"ramen\", \"sushi\", \"hamburger\"],\n        \"calories\": [300, 285, 450, 200, 350],\n        \"description\": [\n            \"dessert with apples\",\n            \"tomato sauce & cheese\",\n            \"noodle soup\",\n            \"vinegared rice\",\n            \"bun with patty\"\n        ]\n    }\n    nutrition_df = pd.DataFrame(data_map)\n    db = build_image_database(parent_dataset_path, local_subset_path, clip_m, clip_p, nutrition_df, 5000, device)\n\n    all_subset_imgs = glob.glob(os.path.join(local_subset_path, \"*\", \"*.jpg\"))\n    if not all_subset_imgs:\n        print(\"No images found.\")\n        return\n    candidate_img = random.choice(all_subset_imgs)\n    top_similar = db.search_similar(candidate_img, top_k=3)\n    outcome = analyze_food_image(candidate_img, blip_mod, blip_proc, top_similar, device)\n    print(\"RESULT\")\n    print(outcome)\n    for idx, item in enumerate(top_similar):\n        print(f\"{idx+1}. {item['path']} {item['similarity']:.3f}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Extra Visualization","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport glob\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torch\nfrom transformers import (\n    BlipProcessor,\n    BlipForConditionalGeneration\n)\n\ndef display_image(image_path):\n    if not os.path.exists(image_path):\n        print(f\"Not found {image_path}.\")\n        return\n    with Image.open(image_path) as img:\n        plt.imshow(img)\n        plt.axis(\"off\")\n        plt.show()\n\ndef get_random_image(image_list):\n    if not image_list:\n        raise ValueError(\"Empty list\")\n    return random.choice(image_list)\n\ndef quick_caption_check(image_path, blip_model, blip_processor, device):\n    try:\n        with Image.open(image_path) as img:\n            inputs = blip_processor(img, return_tensors=\"pt\").to(device)\n            with torch.no_grad():\n                caption_ids = blip_model.generate(**inputs, max_new_tokens=50)\n            caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n        print(f\"\\nImage: {image_path}\\nCaption: {caption}\\n\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n\ndef main():\n    subset_dir = \"/kaggle/working/food101_subset\"\n    files = glob.glob(os.path.join(subset_dir, \"*\", \"*.jpg\"))\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    blip_proc = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n    blip_mod = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n\n    try:\n        picked = get_random_image(files)\n        display_image(picked)\n        quick_caption_check(picked, blip_mod, blip_proc, device)\n    except ValueError as e:\n        print(f\"Error: {e}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}